{
  "videoId": "krsuaUp__pM",
  "title": "System Design Interview: Design a Web Crawler w/ a Ex-Meta Staff Engineer",
  "keyword": "operational transformation",
  "explanation": "## operational transformation\n\nOperational Transformation (OT) is a technique used in distributed systems to enable real-time collaborative editing by allowing multiple users to make changes concurrently. It ensures that all users see a consistent state of the document by transforming operations based on the context of other operations that have been applied, thereby resolving conflicts seamlessly.\n\n### How it applies in this context:\nIn the context of designing a **Web Crawler**, operational transformation can be relevant when multiple instances of the crawler are running concurrently, each processing different URLs. If they need to share or synchronize their state (like the list of URLs to crawl or the data extracted), OT can help manage these updates efficiently, ensuring that all instances reflect the most current state without conflicts.\n\n### Summary Breakdown:\n- **Web Crawler**: A system designed to traverse the web and extract **text data** from pages.\n- **Functional Requirements**: Ability to **crawl** from seed URLs and **store** extracted data.\n- **Non-functional Requirements**: Must be **scalable** to handle 10 billion pages and complete within a **five-day** window.\n- **Fault Tolerance**: Implementing strategies like **automatic retries** and **checkpoints** to maintain state.\n- **Data Flow**: Involves a **frontier queue** for managing URLs and **worker nodes** for processing.\n- **Compliance**: Adhering to **robots.txt** and other web scraping policies for ethical data extraction.\n\n### Diagram:\n```mermaid\ngraph TD\n    A[Seed URLs] --> B[Frontier Queue]\n    B --> C[Crawler Workers]\n    C --> D[Fetch HTML Content]\n    D --> E[Extract Text Data]\n    E --> F[Store in Blob Storage]\n    C --> G[Requeue Discovered URLs]\n    F --> H[Database for URL Metadata]\n    H --> I[Check Compliance with robots.txt]\n```\n\n### Code Example (if applicable):\n```javascript\nclass WebCrawler {\n    constructor(seedUrls) {\n        this.frontierQueue = seedUrls;\n        this.extractedData = [];\n    }\n\n    async crawl() {\n        while (this.frontierQueue.length > 0) {\n            const url = this.frontierQueue.shift();\n            try {\n                const htmlContent = await this.fetchPage(url);\n                const textData = this.extractText(htmlContent);\n                this.extractedData.push({ url, textData });\n                this.requeueUrls(htmlContent);\n            } catch (error) {\n                console.error(`Failed to fetch ${url}: ${error}`);\n                this.frontierQueue.push(url); // Retry logic\n            }\n        }\n    }\n\n    async fetchPage(url) {\n        // Simulated fetch operation\n        return new Promise((resolve, reject) => {\n            setTimeout(() => {\n                if (Math.random() > 0.1) { // Simulating a 90% success rate\n                    resolve(`<html>Content of ${url}</html>`);\n                } else {\n                    reject('Network error');\n                }\n            }, 100);\n        });\n    }\n\n    extractText(html) {\n        // Simple text extraction logic\n        return html.replace(/<[^>]*>/g, ''); // Remove HTML tags\n    }\n\n    requeueUrls(html) {\n        // Simulated URL extraction logic\n        const newUrls = ['http://example.com/page1', 'http://example.com/page2'];\n        this.frontierQueue.push(...newUrls);\n    }\n}\n```\n\nThis code example demonstrates a basic structure of a web crawler that fetches pages, extracts text, and handles retries in case of failures, aligning with the principles discussed in the context.",
  "model": "gpt-4o-mini",
  "generatedAt": "2025-07-14T15:22:17.942Z"
}