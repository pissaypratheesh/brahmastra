{
  "videoId": "note-md1zo4tynwuffe68ilg",
  "title": "System Design Interview: Design a Web Crawler w/ a Ex-Meta Staff Engineer",
  "keyword": "web-crawler",
  "explanation": "## web-crawler\n\nA **web crawler** is an automated program or script designed to browse the internet systematically, retrieving and indexing content from web pages. It starts from a set of initial URLs, follows hyperlinks, and extracts relevant data, which can be used for various applications such as search engines, data mining, or training machine learning models.\n\n### How it applies in this context:\nIn the provided context, the web crawler is designed to extract text data from billions of web pages to train a large language model (LLM) like ChatGPT. The design considerations include handling a vast amount of data efficiently within a limited timeframe while adhering to web standards and ensuring fault tolerance.\n\n### Summary Breakdown:\n- **Goal**: Design a **web crawler** to extract **text data** from the web for training a **large language model (LLM)**.\n- **Functional Requirements**: The crawler must start from **seed URLs**, extract data, and store it efficiently.\n- **Non-Functional Requirements**: Emphasizes **fault tolerance**, **politeness** (adhering to **robots.txt**), and **scalability** to handle up to **10 billion web pages**.\n- **High-Level Design**: Utilizes a **queue system** (like Kafka or SQS) for managing URLs and a **worker system** for fetching and processing content.\n- **Storage System**: Employs **blob storage** (e.g., S3) for storing the extracted text data.\n\n### Diagram:\n```mermaid\ngraph TD\n    A[Start with Seed URLs] --> B[Resolve URL DNS]\n    B --> C[Fetch Pages]\n    C --> D[Extract Text and URLs]\n    D --> E[Store Extracted Text]\n    E --> F[Update Frontier Queue]\n    F --> B\n    F --> G[Adhere to robots.txt]\n    G --> C\n```\n\n### Code Example (if applicable):\n```javascript\nconst axios = require('axios');\nconst cheerio = require('cheerio');\n\nasync function crawl(url) {\n    try {\n        const response = await axios.get(url);\n        const html = response.data;\n        const $ = cheerio.load(html);\n        \n        // Extract text\n        const text = $('body').text();\n        \n        // Extract links\n        const links = [];\n        $('a').each((i, element) => {\n            links.push($(element).attr('href'));\n        });\n\n        console.log(`Extracted text from ${url}:`, text);\n        return links;\n    } catch (error) {\n        console.error(`Error fetching ${url}:`, error);\n        return [];\n    }\n}\n\n// Example usage\nconst seedUrl = 'https://example.com';\ncrawl(seedUrl).then(links => {\n    console.log('Found links:', links);\n});\n``` \n\nThis code snippet demonstrates a simple web crawler that uses `axios` to fetch a webpage and `cheerio` to extract text and hyperlinks from the HTML content.",
  "model": "gpt-4o-mini",
  "generatedAt": "2025-07-14T02:02:55.992Z"
}