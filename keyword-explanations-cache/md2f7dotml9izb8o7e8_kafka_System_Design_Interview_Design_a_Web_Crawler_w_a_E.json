{
  "videoId": "md2f7dotml9izb8o7e8",
  "title": "System Design Interview: Design a Web Crawler w/ a Ex-Meta Staff Engineer",
  "keyword": "kafka",
  "explanation": "## kafka\n\nApache Kafka is a distributed event streaming platform capable of handling real-time data feeds. It is designed to provide high throughput, fault tolerance, and scalability, making it ideal for building real-time data pipelines and streaming applications. Kafka operates as a publish-subscribe messaging system, allowing producers to send messages to topics, which consumers can then read from, facilitating asynchronous communication between different components of a system.\n\n### How it applies in this context:\nIn the context of designing a web crawler, Kafka can be utilized as a **message queue** to manage the flow of **URLs** that need to be crawled. By storing seed URLs in Kafka, the crawler workers can efficiently pull URLs for processing, ensuring that the system can scale horizontally and handle spikes in traffic while maintaining fault tolerance and data integrity.\n\n### Summary Breakdown:\n- A **web crawler** systematically browses the internet to **index** and extract information from web pages.\n- The system must handle up to **10 billion pages**, ensuring **scalability** and **efficiency** within a five-day timeframe.\n- **Fault tolerance** strategies include **retry mechanisms** and **checkpoints** to maintain data integrity during failures.\n- The architecture involves a **Frontier Queue** for managing URLs, **crawler workers** for fetching and processing data, and **blob storage** for long-term data storage.\n- Compliance with **web standards** such as **Robots.txt** is essential to respect website policies during crawling.\n\n### Diagram:\n```mermaid\ngraph TD\n    A[Seed URLs] -->|Push to| B[Kafka Queue]\n    B -->|Pull from| C[Crawler Workers]\n    C -->|Fetch Content| D[Web Pages]\n    D -->|Extract Data| E[Structured Data]\n    E -->|Store in| F[Blob Storage]\n    C -->|Discover New URLs| B\n```\n\n### Code Example (if applicable):\n```javascript\nconst { Kafka } = require('kafkajs');\n\nconst kafka = new Kafka({\n  clientId: 'web-crawler',\n  brokers: ['localhost:9092']\n});\n\nconst producer = kafka.producer();\n\nconst run = async () => {\n  await producer.connect();\n  await producer.send({\n    topic: 'seed-urls',\n    messages: [\n      { value: 'http://example.com' },\n      { value: 'http://another-example.com' }\n    ],\n  });\n  await producer.disconnect();\n};\n\nrun().catch(console.error);\n```\n\nThis code snippet demonstrates how to produce messages to a Kafka topic named `seed-urls`, which can be used by crawler workers to fetch and process URLs.",
  "model": "gpt-4o-mini",
  "generatedAt": "2025-07-14T03:25:35.885Z"
}