{
  "videoId": "krsuaUp__pM",
  "title": "System Design Interview: Design a Web Crawler w/ a Ex-Meta Staff Engineer",
  "keyword": "blob storage",
  "explanation": "## blob storage\n\nBlob storage is a type of data storage designed to store large amounts of unstructured data, such as text or binary data. It allows for the efficient storage and retrieval of data, making it ideal for applications that require scalability, durability, and high availability, such as media files, backups, and large datasets.\n\n### How it applies in this context:\nIn the context of the web crawler system design, **blob storage** is utilized for the long-term storage of extracted text data from crawled web pages. By leveraging blob storage solutions like AWS S3, the system can efficiently manage and scale the storage of vast amounts of data, ensuring that the crawled information is easily accessible for further processing or analysis.\n\n### Summary Breakdown:\n- The web crawler's goal is to **extract** and **store** text data from the web, starting from **seed URLs**.\n- The system must handle **10 billion pages** efficiently within a **five-day** timeframe, emphasizing **scalability** and **fault tolerance**.\n- **Blob storage** is used to store the extracted text data, allowing for **durability** and **high availability**.\n- The architecture includes components like the **Frontier Queue**, **Crawler Workers**, and a **Database** for managing URL states and metadata.\n- Compliance with web scraping norms is ensured through respect for **robots.txt** and implementing **TLS** for secure data transmission.\n\n### Diagram:\n```mermaid\ngraph TD\n    A[Seed URLs] --> B[Frontier Queue]\n    B --> C[Crawler Workers]\n    C --> D[Fetch HTML Content]\n    D --> E[Extract Text Data]\n    E --> F[Blob Storage]\n    E --> G[Database]\n    G --> H[URL Metadata]\n    F --> I[Stored Text Data]\n```\n\n### Code Example (if applicable):\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3();\n\nasync function uploadToBlobStorage(bucketName, key, data) {\n    const params = {\n        Bucket: bucketName,\n        Key: key,\n        Body: data,\n        ContentType: 'text/plain'\n    };\n\n    try {\n        const response = await s3.upload(params).promise();\n        console.log(`File uploaded successfully at ${response.Location}`);\n    } catch (error) {\n        console.error('Error uploading file:', error);\n    }\n}\n\n// Example usage\nconst bucketName = 'my-crawler-data';\nconst key = 'extracted-text/page1.txt';\nconst data = 'This is the extracted text from page 1.';\nuploadToBlobStorage(bucketName, key, data);\n```",
  "model": "gpt-4o-mini",
  "generatedAt": "2025-07-14T15:21:26.968Z"
}