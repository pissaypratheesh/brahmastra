{
  "id": "md1zo4tynwuffe68ilg",
  "topic": "System Design Interview: Design a Web Crawler w/ a Ex-Meta Staff Engineer",
  "content": "## üìã Table of Contents\n- [1. Filtered Video Resources](#filtered-video-resources)\n- [2. Video Overview](#video-overview)\n- [3. Detailed Technical Content](#detailed-technical-content)\n  - [3.1 Problem Definition](#problem-definition)\n  - [3.2 Functional Requirements](#functional-requirements)\n  - [3.3 Non-Functional Requirements](#non-functional-requirements)\n  - [3.4 System Design Roadmap](#system-design-roadmap)\n  - [3.5 High-Level Design](#high-level-design)\n  - [3.6 Deep Dives](#deep-dives)\n- [4. Key Takeaways](#key-takeaways)\n\n---\n\n# Filtered Video Resources\n- **Detailed Write-Up of the Problem**: [Web Crawler Design Guide](https://www.hellointerview.com/learn/system-design/answer-keys/web-crawler)\n- **System Design In a Hurry**: [Quick Introduction](https://www.hellointerview.com/learn/system-design/in-a-hurry/introduction)\n- **Excalidraw Diagrams**: [Excalidraw Link](https://link.excalidraw.com/l/56zGeHiLyKZ/9hC265dNFoo)\n- **Related Video Breakdowns**: \n  - [Ticketmaster System Design](https://www.youtube.com/watch?v=fhdPyoO6aXI)\n  - [Uber System Design](https://www.youtube.com/watch?v=lsKU38RKQSo)\n  - [Dropbox System Design](https://www.youtube.com/watch?v=_UZ1ngy-kOI)\n  - [Ad Click Aggregator Design](https://youtu.be/Zcv_899yqhI)\n\n[‚¨ÜÔ∏è Back to Top](#table-of-contents)\n\n---\n\n# Video Overview\nThis video serves as a comprehensive guide on designing a web crawler, a prevalent system design interview question posed by leading tech companies such as Meta, Google, Amazon, and Microsoft. Evan, a former Staff Engineer at Meta and co-founder of Hello Interview, shares invaluable insights on how to effectively tackle this problem in an interview scenario. This resource is specifically directed at aspiring software engineers preparing for interviews in high-pressure environments like FAANG (Facebook, Amazon, Apple, Netflix, Google) companies, emphasizing not only the technical knowledge needed but also the strategic thinking required during system design discussions.\n\n[‚¨ÜÔ∏è Back to Top](#table-of-contents)\n\n---\n\n# Detailed Technical Content\n\n## 3.1 Problem Definition\n- **Goal**: Design a robust web crawler capable of efficiently extracting text data from the vastness of the internet to facilitate the training of a large language model (LLM) like ChatGPT. The performance of LLMs is heavily dependent on the quality and quantity of data used during their training phases.\n- **Constraints**: The crawler has a strict operational limit of five days and must efficiently handle a scale of up to 10 billion web pages, with each page averaging 2 MB in size. This introduces challenges regarding both speed and storage.\n- **Resources**: Assume access to significant computational resources, ideally involving a distributed system architecture to facilitate scalability and redundancy.\n\n## 3.2 Functional Requirements\n- The crawler must initiate the web crawling process based on a provided list of seed URLs.\n- Extract and store text data derived from all processed websites.\n- Implement deduplication mechanisms to avoid re-fetching already processed pages.\n- The ability to handle diverse text formats, including HTML, JSON, and XML, ensuring accurate data extraction.\n\n## 3.3 Non-Functional Requirements\n- **Fault Tolerance**: The system must be engineered for robust failure handling to maintain progress and data integrity, incorporating retry mechanisms and backup systems.\n- **Politeness**: Adhere strictly to the directives specified in each site‚Äôs robots.txt file to prevent overwhelming servers with too many requests and mitigate potential legal issues associated with web scraping.\n- **Scalability**: It must be designed to seamlessly scale in response to the number of pages and requests being processed, possibly by employing microservices architecture.\n- **Efficiency**: The system should be optimized to complete the crawling process within the prescribed five-day timeline, limiting latency through concurrency and load balancing strategies.\n\n## 3.4 System Design Roadmap\n- **Core Entities**:\n  - **Text Data**: The primary output consisting of the textual content gathered from web pages.\n  - **URL Metadata**: Comprehensive information pertaining to both crawled URLs and those awaiting processing, including state (pending, complete, failed).\n  - **Domain Metadata**: Meta information regarding domains to ensure compliance with crawling guidelines (e.g., robots.txt adherence).\n\n- **Interface & Data Flow**:\n  - **Input**: The system receives initial seed URLs from which",
  "createdAt": "2025-07-13T18:09:19.222Z",
  "updatedAt": "2025-07-14T02:11:28.434Z",
  "tags": [
    "fault-tolerance"
  ]
}